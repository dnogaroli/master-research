{"cells":[{"cell_type":"markdown","metadata":{"id":"_ZS4lDhzAMkr"},"source":["This notebook scrapes the Brazilian parliament' discourses under the <a href=\"https://www2.camara.leg.br/atividade-legislativa/discursos-e-notas-taquigraficas\" target=\"_blank\"> Discourses and Debates</a> section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YdW_uvoAMkz"},"outputs":[],"source":["import requests\n","from requests import get\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8avJ79EAMk6"},"outputs":[],"source":["def get_html(url):\n","    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n","    r = requests.get(url, headers=headers)\n","    r.encoding = 'UTF-8'\n","    soup = BeautifulSoup(r.text, 'html.parser')\n","    \n","    return soup"]},{"cell_type":"markdown","metadata":{"id":"ilcNxR7YAMk7"},"source":["The code is based on <a href=\"https://github.com/estadao/bolsonaro-e-ditadura-no-congresso/blob/1b53692c67799841f5d1f5a3e0e763ef1ec351a3/code/pega-links.ipynb\" target=\"_blank\">Bolsonaro and dictatorship in the parliament</a> project developed by O Estado de S.Paulo. We have made minor changes for the code to work correctly. Note that you need to manually edit the URLs, making changes on:\n","\n","- **CurrentPage=**: this code scrapes one page at a time\n","- **dtInicio=**: DD/MM/YYYY\n","- **dtFim=**: DD/MM/YYYY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4dRoAOcAMk8"},"outputs":[],"source":["url = 'https://www.camara.leg.br/internet/sitaqweb/resultadoPesquisaDiscursos.asp?CurrentPage=22&BasePesq=plenario&txIndexacao=&txOrador=&txPartido=&dtInicio=01/01/2021&dtFim=31/12/2021&txUF=&txSessao=&listaTipoSessao=&listaTipoInterv=&inFalaPres=&listaTipoFala=&listaFaseSessao=&txAparteante=&listaEtapa=&CampoOrdenacao=dtSessao&TipoOrdenacao=DESC&PageSize=1000&txTexto=&txSumario='"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUvpEm5xAMk9"},"outputs":[],"source":["# collecting page's html\n","html = get_html(url)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ufbdr1XEAMk-"},"outputs":[],"source":["def scrape_table(html):\n","    \n","    # find main the table\n","    table = html.find('table', class_='table table-bordered variasColunas')\n","\n","    # find the headers\n","    headers = table.find_all('th')\n","\n","    # list compre comprehension: creates a list for each header in a dictionary\n","    data = { header.text : [] for header in headers }\n","\n","    # access only the table's body\n","    table_body = table.find('tbody')\n","\n","    # access each row in the table's body\n","    rows = table_body.find_all('tr')\n","        \n","    # filters out stragen lines that don't carry data\n","    rows = [ row for row in rows if not row.has_attr(\"name\") ]\n","\n","    for row in rows:         \n","\n","        # find all table cells\n","        cells = row.find_all('td')\n","\n","        # We will always have eight cells: data, session, fase, discourse, summary, \n","        # speaker, time, and publication. We can save them in list order\n","        \n","        data[\"Data\"].append(cells[0].text)\n","        data[\"Sessão\"].append(cells[1].text)\n","        data[\"Fase\"].append(cells[2].text)\n","        \n","        # Sometimes there are no links to a transcript and this information may be blank. \n","        # So, if the script bumps into a TypeError (by trying to access 'href' of a \n","        # None-type element), the output is to fill it with text.\n","        \n","        try:\n","            data[\"Discurso\"].append(cells[3].find('a')['href'])\n","        except TypeError:\n","            data[\"Discurso\"].append(cells[3].text)\n","        \n","        data[\"Sumário\"].append(cells[4].text) # to have the content: .find('a')['title']\n","        data[\"Orador\"].append(cells[5].text)\n","        data[\"Hora\"].append(cells[6].text)\n","        data[\"Publicação\"].append(cells[7].text)\n","\n","    # remove unnecessary blanks from all lists and make some changes\n","    for key, value in data.items():\n","        data[key] = [ item.strip() for item in value ]\n","\n","        if key == 'Discurso':\n","            new_data = [item.replace(\"\\r\\n\\t\\t\\t\\t\\t\\t\\t\", \"\") for item in value]\n","            new_data = [\"https://www.camara.leg.br/internet/sitaqweb/\" + item if item != '\\xa0' else '-' for item in new_data]\n","            data[key] = new_data\n","            \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TlypE8HAMlB"},"outputs":[],"source":["data = scrape_table(html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"va3LA2G1AMlC"},"outputs":[],"source":["df = pd.DataFrame(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAmMW9QjAMlD"},"outputs":[],"source":["# saving into csv\n","# df.to_csv('2000_04.csv', sep=';', header=True, encoding='utf-8', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laNnzmqBAMlE"},"outputs":[],"source":["# saving into xlsx\n","df.to_excel('2021_22.xlsx', index=False)"]},{"cell_type":"markdown","metadata":{"id":"XtvD9S2NAMlF"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"DQaFXhqyAMlF"},"source":["Use the glob module to retrieve files and save them in an individual file by year."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJ_9rhd4AMlG"},"outputs":[],"source":["path = r'/00_scraping_data' \n","\n","# reading all the excel files\n","filenames = glob.glob(path + '\\*.xlsx')\n","print(filenames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6Dzv7-lAMlI"},"outputs":[],"source":["congress = pd.DataFrame()\n","\n","# to iterate excel file one by one inside the folder\n","for file in filenames:\n","    \n","    # combining multiple excel worksheets into single data frame\n","    df = pd.read_excel(file, sep='\\t', header=0)\n","    \n","    # appending excel files one by one\n","    congress = congress.append(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrmE9IIjAMlI"},"outputs":[],"source":["congress.to_excel(r'21_2021.xlsx', header=True, encoding='utf-8', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"00_getting_links.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}